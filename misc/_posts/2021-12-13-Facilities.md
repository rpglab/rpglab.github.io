---
layout: misc
title: Facilities
---

<div class="smallhead mb-1" style="font-size:18px;">
      <p>
This page introduces the facilities that the RPG Lab owns or has access to.
      </p>
</div>


## RPG Lab
We have *<u>six high-end servers</u>* that will provide computing resources. Two of them are equipped with high-performance GPUs. Their specifications are listed below:
* *<u>Server 1</u>*: Intel Xeon W-2195 2.3GHz, 4.3GHz Turbo, 18 Cores, 24.75MB Cache, 8TB SATA Hard Drive, 128GB DDR4 Memory.
* *<u>Server 2</u>*: Intel Xeon W-2145 3.7GHz, 4.5GHz Turbo, 8 Cores, 11MB Cache, 4TB SATA Hard Drive, 64GB DDR4 Memory.
* *<u>Server 3</u>*: Intel Xeon W-2195 2.3GHz, 4.3GHz Turbo, 18 Cores, 24.75M Cache, 512GB SSD, 8TB SATA Hard Drive, 128GB DDR4 Memory.
* *<u>Server 4</u>*: Intel Xeon W-2295 3.0GHz, 4.6GHz Turbo, 18 Cores, 24.75M Cache, 512GB SSD, 16TB SATA Hard Drive, 256GB DDR4 Memory.
* *<u>Server 5</u>*: Intel Xeon W-2295 3.0GHz, 4.6GHz Turbo, 18 Cores, 24.75M Cache, 512GB SSD, 16TB SATA Hard Drive, 256GB DDR4 Memory. **GPU**: NVIDIA Quadro RTX 8000, 48GB, 4DP.
* *<u>Server 6</u>*: Intel Xeon W-2295 3.0GHz, 4.6GHz Turbo, 18 Cores, 24.75M Cache, 512GB SSD, 16TB SATA Hard Drive, 256GB DDR4 Memory. **GPU**: NVIDIA Quadro RTX 8000, 48GB, 4DP.

In addition, we also have licenses for many commercial power system software and optimization software including:
* *<u>DSATools</u>*: five floating full-version licenses for PSAT/TSAT/SSAT/VSAT and the ePMU add-on module.
* *<u>PowerWorld</u>*: three full 250,000-bus version licenses and unlimited educational 50-bus version licenses.
* *<u>PSS/E</u>*: four individual licenses; each license supports a user on a computer.
* *<u>AMPL</u>*: three full version AMPL licenses, and multiple licenses for high-performance solvers including Knitro, Cplex, Gurobi, MINOS, BARON, and CONOPT.
* *<u>PSCAD</u>*: PSCAD Educational License – Five User Network License
* *<u>Intel Fortran</u>*: Two floating licenses; each license has 2 seats and each seat supports one server.
* *<u>Microsoft Visio</u>*: Five license for MicroSoft Open Licensing Academic Microsoft Visio 2019 Professional Academic License
In addition to these computers and software tools, this project will have adequate access to other facilities, equipment and resources at the University of Houston (UH) as described below.

<div class="spacer"></div>

## Power Electronics, Microgrids & Subsea Electrical Systems Center (PEMSEC)
The <a class="off" href="https://pemses.ece.uh.edu/facilities/" target="_blank">PEMSEC</a> at UH houses state-of-the-art equipment for both hardware experimentation and real-time simulation. PEMSEC has a number of advanced equipment including Grid Simulator, dSPACE, FPGA Board, and Typhoon HIL Microgid Testbed.

<div class="spacer"></div>

## University-level Software
UH provides an extensive list of software for its faculties and students, including Matlab, Mathematica, SAS, AutoCad, Microsoft Office suites, Acrobat, and Endnote. 

<div class="spacer"></div>

## Engineering Computer Center (ECC)
The <a class="off" href="https://ecc.egr.uh.edu/" target="_blank">ECC</a> is a computing lab set up for Engineering students only. The ECC has 110 computers in 3 labs. The systems all run Windows 10 and also provide access to our Linux servers. These systems are all connected to the campus network and the Internet.

The <a class="off" href="https://ecc.egr.uh.edu/resources/virtual-lab" target="_blank">Engineering Computing Virtual Computer Lab</a> is a platform that enables Engineering students to remotely access specialized software from any computer (both PC and MAC) from anywhere. Our Virtual Lab features Windows based virtual desktops with many Engineering software packages that are currently installed in the ECC. The virtual lab is available via VMWare Horizon Client Software and web browser.

<div class="spacer"></div>

## Libraries
The <a class="off" href="https://libraries.uh.edu/" target="_blank">University of Houston Libraries</a> serves UH students, faculty, staff and the scholarly community. The system consists of MD Anderson Library, the William R. Jenkins Architecture, Design, & Art Library, the Health Sciences Library, and the Music Library. Combined, these four locations provide access to nearly 400 desktop computers and over 100 laptops. With addition of the O’Quinn Law Library, they provide access to more than 3.2 million physical and digital volumes, 159,714 physical and digital journals and other serial subscriptions, 108,834 e-journal titles, 724,075 e-books, and 419 databases.

<div class="spacer"></div>

## Research Computing Data Core (RCDC)

The <a class="off" href="https://uh.edu/rcdc/" target="_blank">Research Computing Data Core (RCDC)</a> is to advance high-performance computing at the University of Houston. These resources are available to faculty, students and staff. Most of the resources are free of charge. RCDC has <a class="off" href="https://uh.edu/rcdc/resources/" target="_blank">*<u>three large clusters</u>*</a>: Opuntia, Sabine, and Carya that are described in detail below.
* *<u>Opuntia</u>*: contains 1,860 cores (within 80 HP Proliant SL 230 compute blades), and 4 HP Proliant SL 250 NVIDIA K40 GPGPU blades. The system is also equipped with 3 large memory nodes – 1 HP Proliant DL 580 with 1 TB of main memory and 2 HP DL 560 each with 512 GB of main memory. The system storage includes a ~600 TB shared file system. Opuntia also provides access to eight nodes containing two NVIDIA GPU’s, giving users access to high-throughput computing and remote visualization capabilities respectively. A 56 Gb/s Ethernet Mellanox switch fabric interconnects the nodes (I/O and compute). Theoretical peak performance is approximately 58 TFlops.
* *<u>Sabine</u>*: contains 5,704 CPU cores in 147 compute and 12 GPU nodes, including four nodes with 8 NVIDIA V100 cards each and a large memory (768GB) node (116 HPE Proliant XL170r nodes, and 8 HPE ProLiant XL190r nodes). Sabine also provides access to eight nodes containing two NVIDIA Pascal GPU’s, giving users access to high-throughput computing and remote visualization capabilities respectively. Sabine has 530 TB of usable NFS shared storage, and 235 TB of Lustre storage for parallel IO applications, and its nodes are connected via Intel OmniPath switch with a 100Gb Line Rate. Theoretical peak performance is approximately 600 TFlops.
* *<u>Carya</u>*: is the latest addition to the RCDC, 2020, offering a total of 208 Hewlett Packard Enterprise compute HPE nodes (ProLiant HPE XL170r and HPE ProLiant DL380) and 64 Nvidia Volta V100 GPUs (Accelerator HPE ProLiant XL270d & XL190r). It contains about 10,000 CPU cores, 327K GPU cores, 45 TB of main memory and 2 TB of high bandwidth GPU memory. Carya nodes are connected via Mellanox HDR Infiniband switch with 100Gb/s Line Rate. This cluster has 1,560 TB of shared hard-disk based storage and 122 TB of shared flash storage space. Theoretical peak performance is approximately 770 Teraflops. 

In addition to the hardware clusters, RCDC has an extensive list of available software packages that RCDC installs, configures, and maintains on their clusters:

* *<u>Compilers</u>*: We have the latest GNU and Intel C/C++/Fortran compiler and PGI compiler suites, as well as Nvidia’s CUDA compiler for GPU computing. We also maintain compilers for other languages as requested including Java.
* *<u>Programming environments</u>*: We maintain several versions of Python, Matlab, and R, as well as several other languages and environments. These environments include many commonly used third-party libraries and packages.
* *<u>Data-processing tools</u>*: These include programs for dealing with large-scale data formats, like HDF5 and NetCDF.
* *<u>Numerical libraries</u>*: These include Intel’s Math Kernel Library (MKL), a set of highly tuned linear algebra routines; the GNU Scientific Library (GSL); FFTW Fourier transform library; and others.
* *<u>Community codes</u>*: We have a large number of commonly-used scientific software packages including codes for molecular dynamics, such as LAMMPS and NAMD; and visualization, such as ParaView; and many more.

